package pomdp.algorithms;
/*
 * Created on May 6, 2005
 *
 * To change the template for this generated file go to
 * Window&gt;Preferences&gt;Java&gt;Code Generation&gt;Code and Comments
 */

/**
 * @author shanigu
 * <p>
 * To change the template for this generated type comment go to
 * Window&gt;Preferences&gt;Java&gt;Code Generation&gt;Code and Comments
 */

import java.util.Iterator;
import java.util.Vector;
import java.util.Map.Entry;

import pomdp.environments.POMDP;
import pomdp.utilities.AlphaVector;
import pomdp.utilities.BeliefState;
import pomdp.utilities.Logger;
import pomdp.utilities.RandomGenerator;
import pomdp.valuefunction.LinearValueFunctionApproximation;
import pomdp.valuefunction.MDPValueFunction;

public abstract class ValueIteration extends PolicyStrategy {
    protected MDPValueFunction m_vfMDP;
    protected LinearValueFunctionApproximation m_vValueFunction;
    protected POMDP m_pPOMDP;
    protected int m_cStates;
    protected int m_cActions;
    protected int m_cObservations;
    protected double m_dGamma;
    protected AlphaVector m_avMaxValues;
    protected final static double MIN_INF = Double.NEGATIVE_INFINITY;
    protected final static double MAX_INF = Double.POSITIVE_INFINITY;
    protected boolean m_bConverged;
    protected double m_dEpsilon = 0.001; //ε 0.001
    protected long m_cElapsedExecutionTime;
    protected long m_cCPUExecutionTime;
    protected long m_cDotProducts;
    protected int m_cValueFunctionChanges;
    protected boolean m_bTerminate;

    protected long m_cTimeInBackup;
    protected long m_cTimeInHV;
    protected long m_cTimeInV;
    protected long m_cAlphaVectorNodes;

    protected static int g_cTrials = 500;
    protected static int g_cStepsPerTrial = 100;

    protected static String m_sBlindPolicyValueFunctionFileName = null; // unused

    protected RandomGenerator m_rndGenerator;


    public ValueIteration(POMDP pomdp) {
        m_pPOMDP = pomdp;
        m_cStates = m_pPOMDP.getStateCount();
        m_cActions = m_pPOMDP.getActionCount();
        m_cObservations = m_pPOMDP.getObservationCount();
        m_dGamma = m_pPOMDP.getDiscountFactor();
        m_avMaxValues = null;
        m_bConverged = false;
        m_cElapsedExecutionTime = 0;
        m_cCPUExecutionTime = 0;
        //m_dFilteredADR = 0.0;
        m_cDotProducts = 0;

        m_cTimeInBackup = 0;
        m_cTimeInHV = 0;
        m_cTimeInV = 0;
        m_cAlphaVectorNodes = 0;

        m_bTerminate = false;

        m_rndGenerator = new RandomGenerator("ValueIteration");

        m_vValueFunction = new LinearValueFunctionApproximation(m_dEpsilon, true);
        m_vfMDP = pomdp.getMDPValueFunction();

        //computeStepsPerTrial();
        initValueFunctionUsingBlindPolicy();

    }

    public abstract void clusterIteration();

    private double findMaxAlphas(int iAction, BeliefState bs, LinearValueFunctionApproximation vValueFunction, AlphaVector[] aNext) {
        AlphaVector avAlpha = null;
        int iObservation = 0;
        double dSumValues = 0.0, dValue = 0, dProb = 0.0, dSumProbs = 0.0;
        BeliefState bsSuccessor = null;

        boolean bCache = m_pPOMDP.getBeliefStateFactory().isCachingBeliefStates();
        for (iObservation = 0; iObservation < m_cObservations; iObservation++) {//����ÿһ���۲죬����������ֵ�������������ر�ֵ
            dProb = bs.probabilityOGivenA(iAction, iObservation);//����ִ�ж���a�õ��۲�o�ĸ���
            dSumProbs += dProb;
            if (dProb > 0.0) {
                bsSuccessor = bs.nextBeliefState(iAction, iObservation);//����b��a��o�ĺ�������
                avAlpha = vValueFunction.getMaxAlpha(bsSuccessor);//����������ֵ������ֵ���ʱ����Ӧ������
                dValue = avAlpha.dotProduct(bsSuccessor);//����������ֵ�����ж�Ӧ�����ֵ
                dSumValues += dValue * dProb;//��Ȩ���
            } else {
                avAlpha = vValueFunction.getLast();
            }
            aNext[iObservation] = avAlpha;//ÿһ��0����һ���Ժ��Ӧ���������

        }

        //ʽF10
        //�ͱ�׼backup�е�ʽ�Ӳ�һ��
        dSumValues /= dSumProbs; //in case due to rounding there is an error and probs do not exactly sum to 1
        dSumValues *= m_pPOMDP.getDiscountFactor();
        dSumValues += m_pPOMDP.immediateReward(bs, iAction); //���������ر�

        m_pPOMDP.getBeliefStateFactory().cacheBeliefStates(bCache);

        return dSumValues;
    }

    private AlphaVector G(int iAction, LinearValueFunctionApproximation vValueFunction, AlphaVector[] aNext) {
        AlphaVector avAlpha = null, avG = null, avSum = null, avResult = null;
        int iObservation = 0;

        //����o���Ѧ����
        for (iObservation = 0; iObservation < m_cObservations; iObservation++) {
            avAlpha = aNext[iObservation];
            avG = avAlpha.G(iAction, iObservation);
            //Logger.getInstance().logln( iObservation + ") " + avAlpha + ", " + avG );
            if (avSum == null)
                avSum = avG.copy();
            else
                avSum.accumulate(avG);
        }

        //���������ر�
        avResult = avSum.addReward(iAction);
        avResult.setAction(iAction);

        avSum.release();
        return avResult;
    }

    public AlphaVector backup(BeliefState bs) {
        return backup(bs, m_vValueFunction);
    }

    public AlphaVector backup(BeliefState bs, LinearValueFunctionApproximation vValueFunction) {

        return backupTauBased(bs, vValueFunction);
    }

    protected AlphaVector backupTauBased(BeliefState bs, LinearValueFunctionApproximation vValueFunction) {
        double dValue = 0.0, dMaxValue = Double.NEGATIVE_INFINITY;
        int iAction = 0, iMaxAction = -1;
        AlphaVector[] aNext = null, aBest = null;

        Vector<AlphaVector[]> vWinners = new Vector<AlphaVector[]>();//������Ϊ��ͬ�����ж��������Ū��vector
        Vector<Integer> vWinnersActions = new Vector<Integer>();//����action

        for (iAction = 0; iAction < m_cActions; iAction++) {//ÿ��action���Ŧ�
            aNext = new AlphaVector[m_cObservations];//aNext�����b��a��V���ҵ�ÿ��o��Ӧ������
            //����ֵΪF10������b��a�̶�ʱ������һ�������õ������value����ÿ��o����һ�����Ӧ������������
            dValue = findMaxAlphas(iAction, bs, vValueFunction, aNext);

            if (dValue > dMaxValue) {
                dMaxValue = dValue;
                vWinners.clear();
                vWinnersActions.clear();
            }
            if (dValue == dMaxValue) {
                aBest = aNext;//aBest�ŵ������action��Ӧ��aNext
                //b��a��o�̶�ʱ�����action�£�ÿ��o����һ�����Ӧ������
                iMaxAction = iAction;
                vWinners.add(aBest);//��¼ȡֵ���Ķ�������Ӧ������
                vWinnersActions.add(iMaxAction);//��¼ȡֵ���Ķ���
            }
        }

        //����ж�����action�����ȡһ��
        int idx = m_rndGenerator.nextInt(vWinners.size());
        aBest = vWinners.elementAt(idx);
        iMaxAction = vWinnersActions.elementAt(idx);
        //��������º�Ħ�
        AlphaVector avMax = G(iMaxAction, vValueFunction, aBest);
        avMax.setWitness(bs);

        return avMax;
    }

    protected double getMaxMinR() {
        return m_pPOMDP.getMaxMinR();
    }

    /**
     * 使用blind策略，初始化值函数LinearValueFunctionApproximation
     */
    protected void initValueFunctionUsingBlindPolicy() {
        m_vValueFunction.clear();

        int iAction = 0, iState = 0, iEndState = 0, iIteration = 0;
        double dValue = 0.0, dNewValue = 0.0, dReward = 0.0, dDiff = 0.0, dMaxDiff = 0.0, dTr = 0.0, dSum = 0.0;
        AlphaVector av = null;
        AlphaVector avNext = null;
        double dMaxResidual = MAX_INF;
        Entry<Integer, Double> entry = null;
        Iterator<Entry<Integer, Double>> itNonZero = null;
        // 初始化为最小值
        LinearValueFunctionApproximation vMin = new LinearValueFunctionApproximation(m_dEpsilon, false);
        initValueFunctionToMin(vMin);

        // 获得概率一致的初始状态
        BeliefState bsUniform = m_pPOMDP.getBeliefStateFactory().getUniformBeliefState();

        if (m_sBlindPolicyValueFunctionFileName != null) { // 这个可能是个什么blind策略文件，但是没有用到
            try {
                m_vValueFunction.load(m_sBlindPolicyValueFunctionFileName, m_pPOMDP);
                Logger.getInstance().logln("Blind policy loaded successfully");
                return;
            } catch (Exception e) {
                Logger.getInstance().logln("Could not load blind policy - " + e);
            }
        }
        Logger.getInstance().logln("Begin blind policy computation  " + m_cActions + " actions");

        // MDP值迭代直至收敛
        for (iAction = 0; iAction < m_cActions; iAction++) {
            av = vMin.elementAt(0); // 取出第一个向量，实际上也只有一个
            //av = m_pPOMDP.newAlphaVector();
            //Logger.getInstance().logln( "Initial " + av );
            iIteration = 0;
            dMaxResidual = MAX_INF;
            // 持续迭代直至收敛，即前后两次最大value的差值小于0.1
            while (dMaxResidual > 0.1) {
                avNext = av.newAlphaVector();
                dMaxDiff = 0.0;
                // 遍历state，对每个state进行一步值迭代，将迭代后的值存入avnext
                // 前一state
                for (iState = 0; iState < m_cStates; iState++) {
                    dSum = 0.0;
                    // 根据可以转移到的后一state
                    itNonZero = m_pPOMDP.getNonZeroTransitions(iState, iAction);
                    while (itNonZero.hasNext()) {
                        entry = itNonZero.next();
                        iEndState = entry.getKey().intValue();
                        dTr = entry.getValue().doubleValue();//ת���ĸ���ֵ
                        dValue = av.valueAt(iEndState);//�������״̬�ĵ�value
                        dSum += dTr * dValue;//
                    }
                    dReward = m_pPOMDP.R(iState, iAction);
                    dNewValue = dReward + dSum * m_dGamma;
                    // 将对应的状态和值存入avNext
                    avNext.setValue(iState, dNewValue);

                    // 更新一步迭代后，差值最大的state
                    dDiff = Math.abs(dNewValue - av.valueAt(iState));
                    if (dDiff > dMaxDiff) {
                        dMaxDiff = dDiff;
                    }
                }

                dMaxResidual = dMaxDiff;
                avNext.finalizeValues();
                // 新的av代替旧的av
                av = avNext;

                iIteration++;

                if (iIteration % 10 == 0)
                    Logger.getInstance().log(".");
            }

            av.setWitness(bsUniform);
            av.setAction(iAction); // 设置对应的动作
            m_vValueFunction.addPrunePointwiseDominated(av); // 取出最大的av
            Logger.getInstance().logln("Done action " + iAction + " after " + iIteration + " iterations |V| = " + m_vValueFunction.size());
        }
        Logger.getInstance().logln("Done blind policy");

        if (m_sBlindPolicyValueFunctionFileName != null) {
            try {
                m_vValueFunction.save(m_sBlindPolicyValueFunctionFileName);
                Logger.getInstance().logln("Blind policy saved successfully");
                return;

            } catch (Exception e) {
                Logger.getInstance().logln("Could not save blind policy - " + e);
            }
        }

    }

    protected void initValueFunctionToMin(LinearValueFunctionApproximation vValueFunction) {
        Logger.getInstance().logln("Init value function to min");
        double dMinValue = getMaxMinR();
        double dDefaultValue = dMinValue / (1 - m_dGamma);

        BeliefState bsUniform = m_pPOMDP.getBeliefStateFactory().getUniformBeliefState();
        Logger.getInstance().logln("Min R value = " + dMinValue + " init value = " + dDefaultValue);

        AlphaVector avMin = null;
        avMin = m_pPOMDP.newAlphaVector();
        avMin.setAllValues(dDefaultValue);
        avMin.finalizeValues();
        avMin.setWitness(bsUniform);
        vValueFunction.add(avMin);
    }

    @SuppressWarnings({"rawtypes", "unchecked"})
    protected Iterator backwardIterator(Vector vElements) {
        Vector vBackward = new Vector();
        int iElement = 0, cElements = vElements.size();
        for (iElement = cElements - 1; iElement >= 0; iElement--) {
            vBackward.add(vElements.get(iElement));
        }
        return vBackward.iterator();
    }


}
